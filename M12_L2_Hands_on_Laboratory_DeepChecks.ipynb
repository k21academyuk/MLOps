{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ“ŒDeepchecks for Data Quality and Model Performance**\n"
      ],
      "metadata": {
        "id": "xjqA465YpWO6"
      },
      "id": "xjqA465YpWO6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Scenario**\n",
        "\n",
        "An insurance company uses a machine learning model to predict fraudulent claims in their dataset. ðŸ’¼\n",
        "\n",
        "This model helps the company detect potential fraud and minimize financial losses. However, the quality of the dataset directly affects the accuracy of the predictions. If the data is incorrect, missing, or inconsistent, the model will be less effective.\n",
        "\n",
        "In this lab, you will work with the **Deepchecks** library to perform data integrity checks on the dataset. You will explore how data quality impacts model performance, including handling missing values, categorical features, and outliers.\n"
      ],
      "metadata": {
        "id": "j51K2JAnpaWB"
      },
      "id": "j51K2JAnpaWB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Description:**\n",
        "\n",
        "In this lab, you will focus on performing data integrity checks and model validation using **Deepchecks**. You will:\n",
        "\n",
        "- Explore the dataset, ensuring it is clean and formatted properly.\n",
        "- Run various integrity checks (like missing values, correlations, and data types).\n",
        "- Validate the model using **Deepchecks**'s built-in model evaluation tools.\n",
        "- Address any issues found in the data to improve model accuracy and reliability.\n",
        "\n",
        "By the end of this lab, you'll gain practical experience in using data validation tools to improve your machine learning models.\n"
      ],
      "metadata": {
        "id": "qEQZ7caLpdOJ"
      },
      "id": "qEQZ7caLpdOJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Why Does Data Integrity Matter?**\n",
        "\n",
        "- **Missing Values**: Missing data can lead to incorrect model predictions if not properly handled. If some information is missing in certain rows or columns, it can skew the analysis and affect the predictions.\n",
        "- **Outliers**: Extreme values (outliers) in data can distort model training, leading to inaccurate results. Outliers can sometimes mislead the model into focusing too much on abnormal cases.\n",
        "- **Feature Correlations**: Highly correlated features might lead to overfitting or multicollinearity, reducing model performance. This means that the model might learn redundant patterns and become too tailored to the training data, causing poor generalization to new data.\n"
      ],
      "metadata": {
        "id": "kU-Y3dw3pigZ"
      },
      "id": "kU-Y3dw3pigZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Problem Statement**\n",
        "\n",
        "If the input data is of low quality, the modelâ€™s predictions will become less reliable. This can lead to poor decision-making and financial losses for the company. Detecting and correcting data issues is key to ensuring model effectiveness.\n"
      ],
      "metadata": {
        "id": "-4So7lEdpkf-"
      },
      "id": "-4So7lEdpkf-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is Deepchecks?**\n",
        "\n",
        "**Deepchecks** is an open-source Python library that helps you ensure data quality and model performance through automated integrity checks. It provides:\n",
        "- Automated checks for data issues like missing values, feature correlations, and outliers.\n",
        "- Model evaluation tools to check for overfitting and performance degradation.\n",
        "\n",
        "By using Deepchecks, you can validate and improve the quality of your datasets, ensuring that the model produces reliable predictions.\n"
      ],
      "metadata": {
        "id": "d5iA3Hlspmtg"
      },
      "id": "d5iA3Hlspmtg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Steps to Analyze and Handle Data Integrity:**\n"
      ],
      "metadata": {
        "id": "AIRqeAe2ppnn"
      },
      "id": "AIRqeAe2ppnn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Install Required Libraries\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are installing the required libraries for **Deepchecks** and other essential Python packages. These libraries provide the tools necessary for data handling, machine learning model evaluation, and performing integrity checks.\n",
        "\n",
        "**Explanation**:\n",
        "- **`deepchecks`**: Library used to run data integrity and model validation checks. This library automates many of the steps to check whether the data is clean and suitable for modeling.\n",
        "- **`ipywidgets`**: Required for interactive widgets to work with notebooks. Widgets will help us visualize the results interactively.\n",
        "- **`pandas`, `numpy`, `scikit-learn`, `joblib`, `scipy`**: Essential libraries for data handling, machine learning, and model evaluation. They provide functions to work with data and implement models.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Note**:  \n",
        "If any errors occur during installation or if you encounter issues with importing the libraries, **restart the kernel** and try running the installation again. This ensures that the libraries are correctly installed and available for use.\n"
      ],
      "metadata": {
        "id": "9TQOylTQpt2k"
      },
      "id": "9TQOylTQpt2k"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "241a34ec",
      "metadata": {
        "id": "241a34ec",
        "outputId": "cc5e228c-675e-4367-c694-8286394f641d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deepchecks in c:\\users\\supriya\\appdata\\roaming\\python\\python312\\site-packages (0.19.1)\n",
            "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from deepchecks) (2.1.4)\n",
            "Requirement already satisfied: scikit-learn>=0.23.2 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from deepchecks) (1.4.2)\n",
            "Requirement already satisfied: jsonpickle>=2 in c:\\users\\supriya\\appdata\\roaming\\python\\python312\\site-packages (from deepchecks) (4.0.2)\n",
            "Requirement already satisfied: PyNomaly>=0.3.3 in c:\\users\\supriya\\appdata\\roaming\\python\\python312\\site-packages (from deepchecks) (0.3.4)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from deepchecks) (4.11.0)\n",
            "Requirement already satisfied: tqdm>=4.62.3 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from deepchecks) (4.66.5)\n",
            "Requirement already satisfied: category-encoders>=2.3.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from deepchecks) (2.7.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from deepchecks) (1.11.3)\n",
            "Requirement already satisfied: plotly>=5.13.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from deepchecks) (5.24.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from deepchecks) (3.7.5)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from deepchecks) (4.12.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from deepchecks) (2.32.3)\n",
            "Requirement already satisfied: statsmodels>=0.13.5 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from deepchecks) (0.14.2)\n",
            "Requirement already satisfied: numpy>=1.22.2 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from deepchecks) (1.26.4)\n",
            "Requirement already satisfied: ipython>=7.15.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from deepchecks) (8.27.0)\n",
            "Requirement already satisfied: ipykernel>=5.3.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from deepchecks) (6.28.0)\n",
            "Requirement already satisfied: ipywidgets>=7.6.5 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from deepchecks) (7.8.1)\n",
            "Requirement already satisfied: jupyter-server>=2.7.2 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from deepchecks) (2.15.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->deepchecks) (2.5)\n",
            "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from category-encoders>=2.3.0->deepchecks) (0.5.6)\n",
            "Requirement already satisfied: comm>=0.1.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipykernel>=5.3.0->deepchecks) (0.2.1)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipykernel>=5.3.0->deepchecks) (1.6.7)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipykernel>=5.3.0->deepchecks) (8.6.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipykernel>=5.3.0->deepchecks) (5.7.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipykernel>=5.3.0->deepchecks) (0.1.6)\n",
            "Requirement already satisfied: nest-asyncio in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipykernel>=5.3.0->deepchecks) (1.6.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipykernel>=5.3.0->deepchecks) (24.1)\n",
            "Requirement already satisfied: psutil in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipykernel>=5.3.0->deepchecks) (5.9.0)\n",
            "Requirement already satisfied: pyzmq>=24 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipykernel>=5.3.0->deepchecks) (25.1.2)\n",
            "Requirement already satisfied: tornado>=6.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipykernel>=5.3.0->deepchecks) (6.4.1)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipykernel>=5.3.0->deepchecks) (5.14.3)\n",
            "Requirement already satisfied: decorator in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipython>=7.15.0->deepchecks) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipython>=7.15.0->deepchecks) (0.19.1)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipython>=7.15.0->deepchecks) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipython>=7.15.0->deepchecks) (2.15.1)\n",
            "Requirement already satisfied: stack-data in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipython>=7.15.0->deepchecks) (0.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipython>=7.15.0->deepchecks) (0.4.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipywidgets>=7.6.5->deepchecks) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.6 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipywidgets>=7.6.5->deepchecks) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets<3,>=1.0.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipywidgets>=7.6.5->deepchecks) (1.0.0)\n",
            "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server>=2.7.2->deepchecks) (4.2.0)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server>=2.7.2->deepchecks) (21.3.0)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server>=2.7.2->deepchecks) (3.1.4)\n",
            "Requirement already satisfied: jupyter-events>=0.11.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server>=2.7.2->deepchecks) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server>=2.7.2->deepchecks) (0.4.4)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server>=2.7.2->deepchecks) (7.16.4)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server>=2.7.2->deepchecks) (5.10.4)\n",
            "Requirement already satisfied: overrides>=5.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server>=2.7.2->deepchecks) (7.4.0)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server>=2.7.2->deepchecks) (0.14.1)\n",
            "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server>=2.7.2->deepchecks) (2.0.10)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server>=2.7.2->deepchecks) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server>=2.7.2->deepchecks) (0.17.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server>=2.7.2->deepchecks) (1.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.4->deepchecks) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.4->deepchecks) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.4->deepchecks) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.4->deepchecks) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.4->deepchecks) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.4->deepchecks) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.4->deepchecks) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->deepchecks) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->deepchecks) (2023.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from plotly>=5.13.1->deepchecks) (8.2.3)\n",
            "Requirement already satisfied: python-utils in c:\\users\\supriya\\appdata\\roaming\\python\\python312\\site-packages (from PyNomaly>=0.3.3->deepchecks) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from requests>=2.22.0->deepchecks) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from requests>=2.22.0->deepchecks) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from requests>=2.22.0->deepchecks) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from requests>=2.22.0->deepchecks) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23.2->deepchecks) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23.2->deepchecks) (3.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server>=2.7.2->deepchecks) (1.3.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server>=2.7.2->deepchecks) (21.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=7.15.0->deepchecks) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jinja2>=3.0.3->jupyter-server>=2.7.2->deepchecks) (2.1.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=5.3.0->deepchecks) (3.10.0)\n",
            "Requirement already satisfied: pywin32>=300 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=5.3.0->deepchecks) (305.1)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.7.2->deepchecks) (4.23.0)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server>=2.7.2->deepchecks) (2.0.7)\n",
            "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server>=2.7.2->deepchecks) (6.0.1)\n",
            "Requirement already satisfied: referencing in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server>=2.7.2->deepchecks) (0.30.2)\n",
            "Requirement already satisfied: rfc3339-validator in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server>=2.7.2->deepchecks) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server>=2.7.2->deepchecks) (0.1.1)\n",
            "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=2.7.2->deepchecks) (4.1.0)\n",
            "Requirement already satisfied: defusedxml in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=2.7.2->deepchecks) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=2.7.2->deepchecks) (0.1.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=2.7.2->deepchecks) (2.0.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=2.7.2->deepchecks) (0.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=2.7.2->deepchecks) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=2.7.2->deepchecks) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server>=2.7.2->deepchecks) (2.16.2)\n",
            "Requirement already satisfied: six in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from patsy>=0.5.1->category-encoders>=2.3.0->deepchecks) (1.16.0)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.15.0->deepchecks) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.6.6->ipywidgets>=7.6.5->deepchecks) (7.2.2)\n",
            "Requirement already satisfied: executing in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.15.0->deepchecks) (0.8.3)\n",
            "Requirement already satisfied: asttokens in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.15.0->deepchecks) (2.0.5)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.15.0->deepchecks) (0.2.2)\n",
            "Requirement already satisfied: webencodings in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server>=2.7.2->deepchecks) (0.5.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.7.2->deepchecks) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.7.2->deepchecks) (2023.7.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.7.2->deepchecks) (0.10.6)\n",
            "Requirement already satisfied: fqdn in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.7.2->deepchecks) (1.5.1)\n",
            "Requirement already satisfied: isoduration in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.7.2->deepchecks) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.7.2->deepchecks) (2.1)\n",
            "Requirement already satisfied: uri-template in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.7.2->deepchecks) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.7.2->deepchecks) (24.11.1)\n",
            "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets>=7.6.5->deepchecks) (2.27.3)\n",
            "Requirement already satisfied: jupyterlab<4.3,>=4.2.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets>=7.6.5->deepchecks) (4.2.5)\n",
            "Requirement already satisfied: notebook-shim<0.3,>=0.2 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets>=7.6.5->deepchecks) (0.2.4)\n",
            "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=2.7.2->deepchecks) (1.17.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=2.7.2->deepchecks) (2.21)\n",
            "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets>=7.6.5->deepchecks) (2.0.4)\n",
            "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets>=7.6.5->deepchecks) (0.27.0)\n",
            "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets>=7.6.5->deepchecks) (2.2.5)\n",
            "Requirement already satisfied: setuptools>=40.1.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets>=7.6.5->deepchecks) (75.8.0)\n",
            "Requirement already satisfied: babel>=2.10 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets>=7.6.5->deepchecks) (2.11.0)\n",
            "Requirement already satisfied: json5>=0.9.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets>=7.6.5->deepchecks) (0.9.6)\n",
            "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.7.2->deepchecks) (1.2.3)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets>=7.6.5->deepchecks) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets>=7.6.5->deepchecks) (0.14.0)\n",
            "Requirement already satisfied: ipywidgets in c:\\users\\supriya\\anaconda3\\lib\\site-packages (7.8.1)\n",
            "Requirement already satisfied: comm>=0.1.3 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.6 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipywidgets) (3.6.6)\n",
            "Requirement already satisfied: ipython>=4.0.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipywidgets) (8.27.0)\n",
            "Requirement already satisfied: jupyterlab-widgets<3,>=1.0.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipywidgets) (1.0.0)\n",
            "Requirement already satisfied: decorator in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (2.15.1)\n",
            "Requirement already satisfied: stack-data in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.4.6)\n",
            "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.6.6->ipywidgets) (7.2.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.15.0)\n",
            "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.27.3)\n",
            "Requirement already satisfied: jupyterlab<4.3,>=4.2.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.2.5)\n",
            "Requirement already satisfied: notebook-shim<0.3,>=0.2 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (6.4.1)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
            "Requirement already satisfied: executing in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: asttokens in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (2.0.5)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.2.0)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (21.3.0)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (3.1.4)\n",
            "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (8.6.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (5.7.2)\n",
            "Requirement already satisfied: jupyter-events>=0.11.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.4.4)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (7.16.4)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: overrides>=5.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (7.4.0)\n",
            "Requirement already satisfied: packaging>=22.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (24.1)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.14.1)\n",
            "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.0.10)\n",
            "Requirement already satisfied: pyzmq>=24 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (25.1.2)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.17.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.0.4)\n",
            "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.27.0)\n",
            "Requirement already satisfied: ipykernel>=6.5.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (6.28.0)\n",
            "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.2.5)\n",
            "Requirement already satisfied: setuptools>=40.1.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (75.8.0)\n",
            "Requirement already satisfied: babel>=2.10 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.11.0)\n",
            "Requirement already satisfied: json5>=0.9.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.9.6)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.23.0)\n",
            "Requirement already satisfied: requests>=2.31 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.32.3)\n",
            "Requirement already satisfied: six in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from asttokens->stack-data->ipython>=4.0.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.3.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: pytz>=2015.7 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from babel>=2.10->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2024.1)\n",
            "Requirement already satisfied: certifi in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.14.0)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.6.7)\n",
            "Requirement already satisfied: nest-asyncio in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (5.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jinja2>=3.0.3->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.10.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (3.10.0)\n",
            "Requirement already satisfied: pywin32>=300 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (305.1)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.0.7)\n",
            "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (6.0.1)\n",
            "Requirement already satisfied: rfc3339-validator in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.1.1)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.1.0)\n",
            "Requirement already satisfied: defusedxml in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.1.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.0.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.2.3)\n",
            "Requirement already satisfied: webencodings in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: fqdn in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: isoduration in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.1)\n",
            "Requirement already satisfied: uri-template in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (24.11.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.5)\n",
            "Requirement already satisfied: pycparser in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.21)\n",
            "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\supriya\\anaconda3\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.2.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install deepchecks\n",
        "!pip install ipywidgets\n",
        "!pip install pandas==1.5.3\n",
        "!pip install numpy\n",
        "!pip install joblib==1.3.0\n",
        "!pip install scikit-learn==1.4.1\n",
        "!pip install scipy==1.11.4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Load and Explore the Data\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are loading the dataset and inspecting its first few rows to understand its structure and contents. This helps us identify the features, data types, and any initial issues such as missing values or incorrect formats.\n",
        "\n",
        "**Explanation**:\n",
        "- **`pandas`**: The `pandas` library is used for data manipulation and analysis. The `pd.read_csv()` function is used to read data from a CSV file and load it into a DataFrame for easy manipulation.\n",
        "- **`file_path`**: This specifies the location of the dataset. The path you provide here points to where the dataset is stored on your local system.\n",
        "- **`sep=';'`**: Specifies the separator used in the file. In this case, it's a semicolon (`;`), which is common for European CSV files.\n",
        "- **`data_df.head()`**: The `head()` function is used to display the first five rows of the dataset. This is useful for quickly checking the structure of the data and verifying that it loaded correctly.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The output will display the first five rows of the dataset. You will see:\n",
        "  - Column names (features)\n",
        "  - The first few entries of the data for each column\n",
        "  - This helps you identify if the dataset has been correctly loaded and if the columns and data types are correct.\n",
        "\n"
      ],
      "metadata": {
        "id": "LnsQsC3NqQkx"
      },
      "id": "LnsQsC3NqQkx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79bc30f1",
      "metadata": {
        "id": "79bc30f1",
        "outputId": "b897c5ec-4643-4669-d1c8-828fbd3b7ecc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Month</th>\n",
              "      <th>WeekOfMonth</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>Make</th>\n",
              "      <th>AccidentArea</th>\n",
              "      <th>DayOfWeekClaimed</th>\n",
              "      <th>MonthClaimed</th>\n",
              "      <th>WeekOfMonthClaimed</th>\n",
              "      <th>Sex</th>\n",
              "      <th>MaritalStatus</th>\n",
              "      <th>...</th>\n",
              "      <th>AgeOfPolicyHolder</th>\n",
              "      <th>PoliceReportFiled</th>\n",
              "      <th>WitnessPresent</th>\n",
              "      <th>AgentType</th>\n",
              "      <th>NumberOfSuppliments</th>\n",
              "      <th>AddressChange_Claim</th>\n",
              "      <th>NumberOfCars</th>\n",
              "      <th>Year</th>\n",
              "      <th>BasePolicy</th>\n",
              "      <th>FraudFound_P</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dec</td>\n",
              "      <td>5</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>Honda</td>\n",
              "      <td>Urban</td>\n",
              "      <td>Tuesday</td>\n",
              "      <td>Jan</td>\n",
              "      <td>1</td>\n",
              "      <td>Female</td>\n",
              "      <td>Single</td>\n",
              "      <td>...</td>\n",
              "      <td>26 to 30</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>External</td>\n",
              "      <td>none</td>\n",
              "      <td>1 year</td>\n",
              "      <td>3 to 4</td>\n",
              "      <td>1994</td>\n",
              "      <td>Liability</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Jan</td>\n",
              "      <td>3</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>Honda</td>\n",
              "      <td>Urban</td>\n",
              "      <td>Monday</td>\n",
              "      <td>Jan</td>\n",
              "      <td>4</td>\n",
              "      <td>Male</td>\n",
              "      <td>Single</td>\n",
              "      <td>...</td>\n",
              "      <td>31 to 35</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>External</td>\n",
              "      <td>none</td>\n",
              "      <td>no change</td>\n",
              "      <td>1 vehicle</td>\n",
              "      <td>1994</td>\n",
              "      <td>Collision</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Oct</td>\n",
              "      <td>5</td>\n",
              "      <td>Friday</td>\n",
              "      <td>Honda</td>\n",
              "      <td>Urban</td>\n",
              "      <td>Thursday</td>\n",
              "      <td>Nov</td>\n",
              "      <td>2</td>\n",
              "      <td>Male</td>\n",
              "      <td>Married</td>\n",
              "      <td>...</td>\n",
              "      <td>41 to 50</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>External</td>\n",
              "      <td>none</td>\n",
              "      <td>no change</td>\n",
              "      <td>1 vehicle</td>\n",
              "      <td>1994</td>\n",
              "      <td>Collision</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jun</td>\n",
              "      <td>2</td>\n",
              "      <td>Saturday</td>\n",
              "      <td>Toyota</td>\n",
              "      <td>Rural</td>\n",
              "      <td>Friday</td>\n",
              "      <td>Jul</td>\n",
              "      <td>1</td>\n",
              "      <td>Male</td>\n",
              "      <td>Married</td>\n",
              "      <td>...</td>\n",
              "      <td>51 to 65</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>External</td>\n",
              "      <td>more than 5</td>\n",
              "      <td>no change</td>\n",
              "      <td>1 vehicle</td>\n",
              "      <td>1994</td>\n",
              "      <td>Liability</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Jan</td>\n",
              "      <td>5</td>\n",
              "      <td>Monday</td>\n",
              "      <td>Honda</td>\n",
              "      <td>Urban</td>\n",
              "      <td>Tuesday</td>\n",
              "      <td>Feb</td>\n",
              "      <td>2</td>\n",
              "      <td>Female</td>\n",
              "      <td>Single</td>\n",
              "      <td>...</td>\n",
              "      <td>31 to 35</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>External</td>\n",
              "      <td>none</td>\n",
              "      <td>no change</td>\n",
              "      <td>1 vehicle</td>\n",
              "      <td>1994</td>\n",
              "      <td>Collision</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  Month  WeekOfMonth  DayOfWeek    Make AccidentArea DayOfWeekClaimed  \\\n",
              "0   Dec            5  Wednesday   Honda        Urban          Tuesday   \n",
              "1   Jan            3  Wednesday   Honda        Urban           Monday   \n",
              "2   Oct            5     Friday   Honda        Urban         Thursday   \n",
              "3   Jun            2   Saturday  Toyota        Rural           Friday   \n",
              "4   Jan            5     Monday   Honda        Urban          Tuesday   \n",
              "\n",
              "  MonthClaimed  WeekOfMonthClaimed     Sex MaritalStatus  ...  \\\n",
              "0          Jan                   1  Female        Single  ...   \n",
              "1          Jan                   4    Male        Single  ...   \n",
              "2          Nov                   2    Male       Married  ...   \n",
              "3          Jul                   1    Male       Married  ...   \n",
              "4          Feb                   2  Female        Single  ...   \n",
              "\n",
              "   AgeOfPolicyHolder PoliceReportFiled WitnessPresent AgentType  \\\n",
              "0           26 to 30                No             No  External   \n",
              "1           31 to 35               Yes             No  External   \n",
              "2           41 to 50                No             No  External   \n",
              "3           51 to 65               Yes             No  External   \n",
              "4           31 to 35                No             No  External   \n",
              "\n",
              "  NumberOfSuppliments  AddressChange_Claim  NumberOfCars  Year  BasePolicy  \\\n",
              "0                none               1 year        3 to 4  1994   Liability   \n",
              "1                none            no change     1 vehicle  1994   Collision   \n",
              "2                none            no change     1 vehicle  1994   Collision   \n",
              "3         more than 5            no change     1 vehicle  1994   Liability   \n",
              "4                none            no change     1 vehicle  1994   Collision   \n",
              "\n",
              "  FraudFound_P  \n",
              "0           No  \n",
              "1           No  \n",
              "2           No  \n",
              "3           No  \n",
              "4           No  \n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "import numpy as np\n",
        "\n",
        "file_path = r'C:\\Users\\Supriya\\Downloads\\carclaims.csv'\n",
        "data_df = pd.read_csv(file_path, sep=';')\n",
        "data_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Set Up Categorical Features and Run Data Integrity Suite\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are defining which columns in the dataset are categorical and then running the **Deepchecks Data Integrity Suite** to check the data for issues such as missing values, outliers, and data type inconsistencies.\n",
        "\n",
        "**Explanation**:\n",
        "- **`categorical_features`**: These are columns in the dataset that contain categorical data. These features include things like `Month`, `Make`, `DayOfWeek`, and `AccidentArea`. By specifying these features, we inform **Deepchecks** that they should be treated as categorical during the integrity check.\n",
        "- **`Dataset`**: The `Dataset` object is used to prepare the data for validation with **Deepchecks**. By passing in the DataFrame (`data_df`) and specifying the categorical features, we are setting up the data for integrity checks.\n",
        "- **`data_integrity()`**: This is a built-in suite in **Deepchecks** that automatically checks for a variety of data quality issues such as missing values, correlations, feature distributions, and more.\n",
        "- **`suite_result.show()`**: After running the integrity checks, the results are displayed. This will show any data quality issues found during the checks.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The output will show the results of the data integrity checks. Possible results could include:\n",
        "  - **Missing Values**: If any columns contain missing values, the suite will flag them, and you may need to handle these by imputation or removal.\n",
        "  - **Feature Correlations**: Highly correlated features may be highlighted. If certain features are highly correlated (e.g., two columns containing similar data), it could lead to multicollinearity issues in model training.\n",
        "  - **Outliers**: Any extreme values in the dataset will be flagged. These may need to be removed or adjusted depending on their impact.\n",
        "  - **Data Type Inconsistencies**: The suite will check if any columns have mixed data types or other irregularities that need to be addressed.\n"
      ],
      "metadata": {
        "id": "17OgzsXvqg-h"
      },
      "id": "17OgzsXvqg-h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "160e47ca",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "8037f61660004651ace787e3b78007e4",
            "641f92a17f1e42f39b9f21d110560592"
          ]
        },
        "id": "160e47ca",
        "outputId": "e1cd69a9-f335-4986-d035-e9975d2323f6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "        progress {\n",
              "            -webkit-appearance: none;\n",
              "            border: none;\n",
              "            border-radius: 3px;\n",
              "            width: 300px;\n",
              "            height: 20px;\n",
              "            vertical-align: middle;\n",
              "            margin-right: 10px;\n",
              "            background-color: aliceblue;\n",
              "        }\n",
              "        progress::-webkit-progress-bar {\n",
              "            border-radius: 3px;\n",
              "            background-color: aliceblue;\n",
              "        }\n",
              "        progress::-webkit-progress-value {\n",
              "            background-color: #9d60fb;\n",
              "        }\n",
              "        progress::-moz-progress-bar {\n",
              "            background-color: #9d60fb;\n",
              "        }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "641f92a17f1e42f39b9f21d110560592",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_FOSUCG8RACRRCUM5620TTM3R9\">Data Integrity Suiâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from deepchecks.tabular import Dataset\n",
        "from deepchecks.tabular.suites import data_integrity\n",
        "\n",
        "# Assuming you know your categorical features\n",
        "categorical_features = ['Month', 'WeekOfMonth', 'DayOfWeek', 'Make', 'AccidentArea', 'DayOfWeekClaimed', 'MonthClaimed']\n",
        "\n",
        "# Initialize the Dataset\n",
        "dataset = Dataset(data_df, cat_features=categorical_features)\n",
        "\n",
        "# Run the data integrity suite on the Dataset object\n",
        "integ_suite = data_integrity()\n",
        "suite_result = integ_suite.run(dataset)  # Using 'dataset' instead of 'data_df'\n",
        "suite_result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5201a19",
      "metadata": {
        "id": "b5201a19"
      },
      "source": [
        "### Step 4: Handle Missing Values and Clean the Data\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are making changes to the dataset by filling missing values, removing specific rows, and transforming certain columns to standardize the data.\n",
        "\n",
        "**Explanation**:\n",
        "- **`data_clean_df = data_df.copy()`**: Creates a copy of the original dataset to ensure that the original data remains unchanged. The new copy, `data_clean_df`, will be used for data cleaning.\n",
        "- **`data_clean_df['PastNumberOfClaims'] = data_clean_df['PastNumberOfClaims'].fillna('none')`**: Replaces missing values (NaN) in the `PastNumberOfClaims` column with the string `'none'`. This is useful if the column has missing data, and `'none'` can be used as a placeholder for cases with no previous claims.\n",
        "- **`data_clean_df.drop(index=1146, inplace=True)`**: Drops the row at index 1146. This might be done because this row contains errors or irrelevant data that should not be included in the analysis.\n",
        "- **`data_clean_df['Month'] = data_clean_df['Month'].replace({str(i): datetime.date(1900, i, 1).strftime('%b') for i in range(1, 13)})`**: This replaces numeric month values (e.g., \"1\", \"2\", etc.) with the corresponding month abbreviations (e.g., \"Jan\", \"Feb\", etc.). It converts month numbers to month names using `datetime` to ensure the data is more readable and consistent.\n",
        "- **`data_clean_df['DayOfWeek'] = data_clean_df['DayOfWeek'].str.lower()`**: Converts all entries in the `DayOfWeek` column to lowercase. This helps standardize the data, making it easier to analyze without worrying about case sensitivity (e.g., \"Monday\" vs \"monday\").\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The dataset `data_clean_df` will now have the missing values in the `PastNumberOfClaims` column filled, the row with index 1146 removed, the `Month` column standardized with month abbreviations, and the `DayOfWeek` column converted to lowercase.\n",
        "- These changes help clean the dataset and prepare it for further analysis or model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02babe0e",
      "metadata": {
        "id": "02babe0e"
      },
      "outputs": [],
      "source": [
        "data_clean_df = data_df.copy()\n",
        "data_clean_df['PastNumberOfClaims'] = data_clean_df['PastNumberOfClaims'].fillna('none')\n",
        "data_clean_df.drop(index=1146, inplace=True)\n",
        "data_clean_df['Month'] = data_clean_df['Month'].replace({str(i): datetime.date(1900, i, 1).strftime('%b') for i in range(1, 13)})\n",
        "data_clean_df['DayOfWeek'] = data_clean_df['DayOfWeek'].str.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Define a Function to Handle String Values and Convert to Numerical Format\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are defining a custom function to convert string values into numerical values. This is especially useful when a column contains mixed types (e.g., numeric values stored as strings or ranges stored as text).\n",
        "\n",
        "**Explanation**:\n",
        "- **`import re`**: Imports the regular expression library, which is used for string matching and manipulation. Here, it's used to extract numbers from a string.\n",
        "- **`str_to_mean(str_val)`**: This is a custom function designed to handle different string values and convert them to numeric values.\n",
        "  - **`if isinstance(str_val, (int, float)):`**: If the value is already a number (either integer or float), it returns the value as is.\n",
        "  - **`if str_val.lower() == 'none':`**: If the string is `'none'` (case insensitive), it returns `np.nan` (Not a Number) to indicate missing data.\n",
        "  - **`if str_val == 'new':`**: If the string is `'new'`, it returns `0`, which might represent a newly created record or a default value.\n",
        "  - **`parts = re.findall(r'\\d+', str_val)`**: Uses a regular expression to find all sequences of digits in the string. For example, if the string is \"1 to 3\", it will extract the numbers `1` and `3`.\n",
        "  - **`parts = list(map(int, parts))`**: Converts the extracted string numbers into integers.\n",
        "  - **`if len(parts) < 1:`**: If no numbers are found, the function raises a `ValueError`, indicating that the string couldn't be processed correctly.\n",
        "  - **`return np.mean(parts)`**: Returns the mean of the extracted numbers. This is useful when the value in the string represents a range (e.g., \"1 to 3\" would be converted to `2`).\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- This function will convert string values in a column (such as \"1 to 3\", \"none\", or \"new\") into numerical values that can be used for further analysis or model training.\n",
        "- For example, a column with strings like `\"2 to 5\"`, `\"none\"`, and `\"new\"` will be converted into numerical values like `3.5` (mean of 2 and 5), `np.nan` (for \"none\"), and `0` (for \"new\").\n",
        "  \n",
        "**Note**: This function is helpful for cleaning up columns with mixed data types and ensuring that the dataset is fully numerical, making it compatible with machine learning models.\n"
      ],
      "metadata": {
        "id": "wp3s5F5aq24I"
      },
      "id": "wp3s5F5aq24I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "040211ec",
      "metadata": {
        "id": "040211ec"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def str_to_mean(str_val):\n",
        "    if isinstance(str_val, (int, float)):\n",
        "        return str_val\n",
        "    if str_val.lower() == 'none':\n",
        "        return np.nan\n",
        "    if str_val == 'new':\n",
        "        return 0\n",
        "    parts = re.findall(r'\\d+', str_val)\n",
        "    parts = list(map(int, parts))\n",
        "    if len(parts) < 1:\n",
        "        raise ValueError(str_val)\n",
        "    return np.mean(parts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Apply the `str_to_mean` Function to Columns\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are applying the `str_to_mean` function to multiple columns in the dataset to convert mixed-type string values into numerical values. This step ensures that all columns are ready for analysis or model training by transforming text-based representations of numbers into actual numeric data.\n",
        "\n",
        "**Explanation**:\n",
        "- **`data_clean_df['VehiclePrice'] = data_clean_df['VehiclePrice'].apply(str_to_mean)`**: This applies the `str_to_mean` function to the `VehiclePrice` column. Any string values like \"new\" or ranges like \"2 to 5\" will be converted into a numerical value (e.g., the average of 2 and 5 for \"2 to 5\").\n",
        "- The same logic applies to the other columns:\n",
        "  - **`Days_Policy_Accident`**\n",
        "  - **`Days_Policy_Claim`**\n",
        "  - **`AgeOfVehicle`**\n",
        "  - **`AgeOfPolicyHolder`**\n",
        "  - **`NumberOfCars`**\n",
        "  - **`PastNumberOfClaims`**\n",
        "\n",
        "Each of these columns is being processed by the `str_to_mean` function to convert mixed text and numerical values into consistent numerical values.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The output will update the `data_clean_df` DataFrame, transforming the columns into numeric values where possible.\n",
        "  - **For example**:\n",
        "    - A value like `\"1 to 3\"` in `VehiclePrice` would be replaced by `2` (mean of 1 and 3).\n",
        "    - `\"none\"` would become `np.nan`, and `\"new\"` would become `0`.\n",
        "\n",
        "**Note**:  \n",
        "This step is crucial because many machine learning models require numeric input. By converting textual data into numerical form, we can ensure that these columns can be used effectively in model training and analysis.\n"
      ],
      "metadata": {
        "id": "5GQnxgESq_mA"
      },
      "id": "5GQnxgESq_mA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c7a7e8f",
      "metadata": {
        "id": "7c7a7e8f"
      },
      "outputs": [],
      "source": [
        "data_clean_df['VehiclePrice'] = data_clean_df['VehiclePrice'].apply(str_to_mean)\n",
        "data_clean_df['Days_Policy_Accident'] = data_clean_df['Days_Policy_Accident'].apply(str_to_mean)\n",
        "data_clean_df['Days_Policy_Claim'] = data_clean_df['Days_Policy_Claim'].apply(str_to_mean)\n",
        "data_clean_df['AgeOfVehicle'] = data_clean_df['AgeOfVehicle'].apply(str_to_mean)\n",
        "data_clean_df['AgeOfPolicyHolder'] = data_clean_df['AgeOfPolicyHolder'].apply(str_to_mean)\n",
        "data_clean_df['NumberOfCars'] = data_clean_df['NumberOfCars'].apply(str_to_mean)\n",
        "data_clean_df['PastNumberOfClaims'] = data_clean_df['PastNumberOfClaims'].apply(str_to_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Handle Categorical Data in `AddressChange_Claim` Column\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are replacing the string values in the `AddressChange_Claim` column with numerical values. This helps convert categorical text data into a form that can be processed by machine learning models. Specifically, we are mapping the number of years or duration to numeric values.\n",
        "\n",
        "**Explanation**:\n",
        "- **`data_clean_df['AddressChange_Claim'] = data_clean_df['AddressChange_Claim'].replace({...})`**: This code replaces specific string values in the `AddressChange_Claim` column with numerical equivalents:\n",
        "  - `'no change'` â†’ `10`\n",
        "  - `'4 to 8 years'` â†’ `6`\n",
        "  - `'2 to 3 years'` â†’ `2.5`\n",
        "  - `'1 year'` â†’ `1`\n",
        "  - `'under 6 months'` â†’ `0.5`\n",
        "- These mappings reflect the durations associated with address changes in the claims data. The goal is to quantify the information so that it can be used in numerical models.\n",
        "- **`.infer_objects()`**: This method is used to infer the data types of the columns. After replacing string values with numeric equivalents, it ensures that the column's data type is appropriately set.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The `AddressChange_Claim` column will now contain numeric values instead of strings, making it suitable for model training. For example:\n",
        "  - `\"no change\"` becomes `10`\n",
        "  - `\"4 to 8 years\"` becomes `6`\n",
        "\n",
        "**Note**:  \n",
        "This transformation is particularly useful when working with machine learning models that expect numerical data. By replacing categorical strings with numeric values, the data becomes compatible with the algorithms.\n"
      ],
      "metadata": {
        "id": "Vd50aOaOrLfB"
      },
      "id": "Vd50aOaOrLfB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b55fcfb",
      "metadata": {
        "id": "6b55fcfb"
      },
      "outputs": [],
      "source": [
        "data_clean_df['AddressChange_Claim'] = data_clean_df['AddressChange_Claim'].replace({\n",
        "    'no change': 10,\n",
        "    '4 to 8 years': 6,\n",
        "    '2 to 3 years': 2.5,\n",
        "    '1 year': 1,\n",
        "    'under 6 months': 0.5\n",
        "}).infer_objects()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8: Group Data by `Year` and `FraudFound_P` and Count the Entries\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are grouping the dataset by `Year` and `FraudFound_P` columns to count the number of occurrences of each combination. This step provides an overview of how the fraud detection data is distributed across different years and fraud outcomes (fraud found or not).\n",
        "\n",
        "**Explanation**:\n",
        "- **`data_clean_df[['Year', 'FraudFound_P', 'Month']]`**: This selects the relevant columns: `Year`, `FraudFound_P` (the target variable indicating whether fraud was found or not), and `Month`.\n",
        "- **`.groupby(['Year', 'FraudFound_P'])`**: Groups the data by the `Year` and `FraudFound_P` columns. This allows us to aggregate the data based on these two features.\n",
        "- **`.count()`**: Counts the number of occurrences in each group. It will return the number of entries for each combination of `Year` and `FraudFound_P`.\n",
        "\n",
        "\n",
        "**Result**:\n",
        "- The output will show a table with counts of entries for each combination of `Year` and `FraudFound_P`. The result will show how many records exist for each combination of year and whether fraud was found or not.\n",
        "\n",
        "**Note**:  \n",
        "This grouping and counting of data helps understand the distribution of fraud cases over the years and assists in preparing the data for model training.\n",
        "\n",
        "ðŸ”½ Run this cell"
      ],
      "metadata": {
        "id": "sGZKODXbrS7y"
      },
      "id": "sGZKODXbrS7y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c8bfe0",
      "metadata": {
        "id": "19c8bfe0",
        "outputId": "937c9d62-938f-44a9-b17d-881adf1ca724"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Month</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Year</th>\n",
              "      <th>FraudFound_P</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1994</th>\n",
              "      <th>No</th>\n",
              "      <td>5732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Yes</th>\n",
              "      <td>409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1995</th>\n",
              "      <th>No</th>\n",
              "      <td>4894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Yes</th>\n",
              "      <td>301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1996</th>\n",
              "      <th>No</th>\n",
              "      <td>3870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Yes</th>\n",
              "      <td>213</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Month\n",
              "Year FraudFound_P       \n",
              "1994 No             5732\n",
              "     Yes             409\n",
              "1995 No             4894\n",
              "     Yes             301\n",
              "1996 No             3870\n",
              "     Yes             213"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_clean_df[['Year', 'FraudFound_P', 'Month']].groupby(['Year', 'FraudFound_P']).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9: Split the Data into Training and Testing Sets\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are splitting the data into a training set and a testing set based on the `Year` column. The training set will contain data from the year 1994, and the testing set will contain data from years after 1994. This is done to simulate a real-world scenario where the model is trained on historical data and tested on newer data.\n",
        "\n",
        "**Explanation**:\n",
        "- **`train_df = data_clean_df[data_clean_df.Year == 1994]`**: This creates the `train_df` DataFrame, which contains all rows from the dataset where the `Year` is 1994. This dataset will be used to train the machine learning model.\n",
        "- **`test_df = data_clean_df[data_clean_df.Year > 1994]`**: This creates the `test_df` DataFrame, which contains all rows from the dataset where the `Year` is greater than 1994. This dataset will be used to test the model on unseen data (post-1994).\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The `train_df` dataset will contain records from 1994, and the `test_df` dataset will contain records from years after 1994. This split helps simulate a scenario where the model is trained on past data and tested on future data to evaluate its generalization.\n"
      ],
      "metadata": {
        "id": "csfyuZCVrgFU"
      },
      "id": "csfyuZCVrgFU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f2d918a",
      "metadata": {
        "id": "9f2d918a"
      },
      "outputs": [],
      "source": [
        "train_df = data_clean_df[data_clean_df.Year == 1994]\n",
        "test_df = data_clean_df[data_clean_df.Year > 1994]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 10: Define Categorical Columns for the Dataset\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are defining a list of categorical columns (`cat_cols`) that will be used for analysis and model training. These columns represent categorical features in the dataset, and identifying them allows us to treat them appropriately during the analysis and modeling process.\n",
        "\n",
        "**Explanation**:\n",
        "- **`cat_cols`**: This is a list of column names in the dataset that contain categorical data. These features include things like `Month`, `DayOfWeek`, `Make`, and `AccidentArea`. These features should be treated as categorical during model training because they represent categories or groups, not continuous numerical values.\n",
        "  \n",
        "By defining these columns explicitly, we can ensure that machine learning models and integrity checks correctly handle them as categorical variables, which is important for both model performance and interpretability.\n",
        "\n"
      ],
      "metadata": {
        "id": "plqsqgxBroUE"
      },
      "id": "plqsqgxBroUE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de86d703",
      "metadata": {
        "id": "de86d703"
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular import Dataset\n",
        "\n",
        "cat_cols = ['Month',\n",
        " 'WeekOfMonth',\n",
        " 'DayOfWeek',\n",
        " 'Make',\n",
        " 'AccidentArea',\n",
        " 'DayOfWeekClaimed',\n",
        " 'MonthClaimed',\n",
        " 'WeekOfMonthClaimed',\n",
        " 'Sex',\n",
        " 'MaritalStatus',\n",
        " 'Fault',\n",
        " 'PolicyType',\n",
        " 'VehicleCategory',\n",
        " 'PoliceReportFiled',\n",
        " 'WitnessPresent',\n",
        " 'AgentType',\n",
        " 'NumberOfSuppliments',\n",
        " 'BasePolicy']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 11: Create Dataset Objects for Training and Testing\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are creating **Deepchecks Dataset** objects for the training and testing datasets. These objects are necessary for running Deepchecksâ€™ data integrity checks, model evaluation, and validation tasks. The dataset objects are configured with information about the label (the target variable), datetime (for time-based validation), and categorical features.\n",
        "\n",
        "**Explanation**:\n",
        "- **`train_ds = Dataset(train_df, label='FraudFound_P', datetime_name='Year', cat_features=cat_cols)`**:\n",
        "  - **`train_df`**: The training dataset containing records from the year 1994.\n",
        "  - **`label='FraudFound_P'`**: Specifies that the target variable (the column we want to predict) is `FraudFound_P`, which indicates whether fraud was found or not.\n",
        "  - **`datetime_name='Year'`**: Specifies that the `Year` column should be treated as the datetime feature, which is useful for time-series validation and analysis.\n",
        "  - **`cat_features=cat_cols`**: Tells Deepchecks which columns should be treated as categorical features. This is important for proper handling during checks and model training.\n",
        "\n",
        "- **`test_ds = Dataset(test_df, label='FraudFound_P', datetime_name='Year', cat_features=cat_cols)`**:\n",
        "  - Creates the **test_ds** dataset object for the testing set (post-1994 data) with the same configuration as the training set.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The `train_ds` and `test_ds` variables will now hold the training and testing dataset objects, respectively. These objects are now ready to be used for running data integrity checks, model evaluation, and further analysis using **Deepchecks**.\n",
        "\n"
      ],
      "metadata": {
        "id": "zUTlszRBr41K"
      },
      "id": "zUTlszRBr41K"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e2e8187",
      "metadata": {
        "id": "8e2e8187"
      },
      "outputs": [],
      "source": [
        "train_ds = Dataset(train_df, label='FraudFound_P', datetime_name='Year', cat_features=cat_cols)\n",
        "test_ds = Dataset(test_df, label='FraudFound_P', datetime_name='Year', cat_features=cat_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 12: Run Train-Test Validation Suite\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are using **Deepchecks'** `train_test_validation` suite to compare the performance of the model on both the training and testing datasets. This suite helps ensure that the model is not overfitting and that its performance is consistent between the training and test datasets.\n",
        "\n",
        "**Explanation**:\n",
        "- **`warnings.filterwarnings(\"ignore\", category=FutureWarning)`**: This line suppresses any warning messages related to future deprecations in Python libraries that may not impact the current code execution. These warnings typically indicate that certain functions or features might be removed or altered in future versions of Python or libraries, but they do not affect the current functionality.\n",
        "  \n",
        "- **`train_test_validation()`**: This is a built-in suite in **Deepchecks** that validates the modelâ€™s performance between the training and test sets. It checks if the model's performance degrades significantly from the training data to the test data, which can indicate overfitting.\n",
        "  \n",
        "- **`res = train_test_validation().run(train_ds, test_ds)`**: Runs the train-test validation on the `train_ds` and `test_ds` dataset objects that were created earlier. The result is stored in `res`.\n",
        "  \n",
        "- **`res.show()`**: Displays the results of the validation. This will include any issues found, such as a significant drop in performance between the training and test sets, indicating overfitting or issues with the data.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The output will show the results of the **train-test validation** suite. This will tell you:\n",
        "  - Whether the modelâ€™s performance on the training set is consistent with its performance on the test set.\n",
        "  - If there is a significant drop in performance, it may indicate that the model is overfitting to the training data.\n",
        "\n",
        "**Note**:  \n",
        "The validation checks for overfitting by comparing performance metrics (e.g., accuracy, F1 score) on the training and test datasets. If there is a large gap in performance, this could indicate that the model has learned to memorize the training data rather than generalize to unseen data.\n"
      ],
      "metadata": {
        "id": "XR4sPQGosCSU"
      },
      "id": "XR4sPQGosCSU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93c76cc7",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "49511cae535444b689fea1202dda1b12",
            "a2e4bb5631164403bb6e2c0121ff64fe"
          ]
        },
        "id": "93c76cc7",
        "outputId": "116ca3be-d554-46e1-8bb0-7cf3e039c798"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "        progress {\n",
              "            -webkit-appearance: none;\n",
              "            border: none;\n",
              "            border-radius: 3px;\n",
              "            width: 300px;\n",
              "            height: 20px;\n",
              "            vertical-align: middle;\n",
              "            margin-right: 10px;\n",
              "            background-color: aliceblue;\n",
              "        }\n",
              "        progress::-webkit-progress-bar {\n",
              "            border-radius: 3px;\n",
              "            background-color: aliceblue;\n",
              "        }\n",
              "        progress::-webkit-progress-value {\n",
              "            background-color: #9d60fb;\n",
              "        }\n",
              "        progress::-moz-progress-bar {\n",
              "            background-color: #9d60fb;\n",
              "        }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2e4bb5631164403bb6e2c0121ff64fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_FOSDAC3VWI782KW79X1KA0C5V\">Train Test Validatâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "from deepchecks.tabular.suites import train_test_validation\n",
        "\n",
        "res = train_test_validation().run(train_ds, test_ds)\n",
        "res.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 13: Remove Specific Car Brands from the Test Dataset\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are filtering out rows from the `test_df` dataset where the `Make` column contains specific car brands ('Ferrari' and 'Lexus'). This may be done to either focus the analysis on more common car brands or remove brands that might be outliers or have unique characteristics in the context of the dataset.\n",
        "\n",
        "**Explanation**:\n",
        "- **`test_df = test_df[~test_df.Make.isin(['Ferrari', 'Lexus'])]`**: This line of code removes rows from `test_df` where the `Make` column is either 'Ferrari' or 'Lexus'.\n",
        "  - **`test_df.Make.isin(['Ferrari', 'Lexus'])`**: This checks if the `Make` column contains 'Ferrari' or 'Lexus'.\n",
        "  - **`~`**: The tilde (`~`) is a negation operator. It inverts the condition, meaning it selects rows where the `Make` is **not** 'Ferrari' or 'Lexus'.\n",
        "  - The result is that rows with these car brands are excluded from `test_df`, ensuring that the model is not influenced by these outliers or niche brands.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The `test_df` dataset will now exclude any rows where the `Make` is either 'Ferrari' or 'Lexus'.\n",
        "- This step may be necessary if these car brands are outliers or do not represent the broader population of cars that the model should generalize to.\n",
        "\n"
      ],
      "metadata": {
        "id": "-G_XSKNYsKRS"
      },
      "id": "-G_XSKNYsKRS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8aaed0a",
      "metadata": {
        "id": "e8aaed0a"
      },
      "outputs": [],
      "source": [
        "test_df = test_df[~test_df.Make.isin(['Ferrari', 'Lexus'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 14: Create Dataset Objects with Additional Configuration\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are re-creating the **Deepchecks Dataset** objects for both the training and testing datasets. This time, we include additional parameters, such as the `index_name`, to uniquely identify each row in the dataset using the `PolicyNumber` column. This step ensures that Deepchecks handles the data correctly with the proper indexing and configuration.\n",
        "\n",
        "**Explanation**:\n",
        "- **`train_ds = Dataset(train_df, label='FraudFound_P', datetime_name='Year', index_name='PolicyNumber', cat_features=cat_cols)`**:\n",
        "  - **`train_df`**: The training dataset.\n",
        "  - **`label='FraudFound_P'`**: Specifies the target variable that the model aims to predict (i.e., whether fraud was found).\n",
        "  - **`datetime_name='Year'`**: Specifies that the `Year` column is the datetime feature, which can be useful for time-based validation.\n",
        "  - **`index_name='PolicyNumber'`**: Specifies that the `PolicyNumber` column should be used as the unique index for each row in the dataset. This is important for tracking each individual policy.\n",
        "  - **`cat_features=cat_cols`**: Defines the categorical features (as previously set).\n",
        "\n",
        "- **`test_ds = Dataset(test_df, label='FraudFound_P', datetime_name='Year', index_name='PolicyNumber', cat_features=cat_cols)`**:\n",
        "  - Creates the **test_ds** dataset object for the test data (post-1994) with the same configuration as the training set.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The `train_ds` and `test_ds` dataset objects will now have the correct configuration for handling categorical features, datetime information, and the unique `PolicyNumber` index.\n",
        "- These objects are now ready to be used for validation, checks, and model training/evaluation within **Deepchecks**.\n",
        "\n",
        "**Note**:  \n",
        "Including the `index_name='PolicyNumber'` is important to ensure that each row is uniquely identified by its policy number, which can be useful in tracking specific policies or for specific validation checks in the context of this dataset.\n"
      ],
      "metadata": {
        "id": "2U9mo3qisS6w"
      },
      "id": "2U9mo3qisS6w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57d9d902",
      "metadata": {
        "id": "57d9d902"
      },
      "outputs": [],
      "source": [
        "train_ds = Dataset(train_df, label='FraudFound_P', datetime_name='Year', index_name='PolicyNumber', cat_features=cat_cols)\n",
        "test_ds = Dataset(test_df, label='FraudFound_P', datetime_name='Year', index_name='PolicyNumber', cat_features=cat_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 15: Run Train-Test Validation Suite Again\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are running the **train-test validation** suite again, but this time using the updated **Deepchecks Dataset** objects, which include the `index_name` (`PolicyNumber`) and other configurations. This step helps validate that the model performs consistently across both the training and testing datasets, ensuring that the model does not overfit.\n",
        "\n",
        "**Explanation**:\n",
        "- **`train_test_validation()`**: This is a **Deepchecks** suite that evaluates whether the model performs similarly on both the training and testing datasets. It checks for overfitting by comparing metrics like accuracy, precision, and recall across both datasets.\n",
        "- **`res = train_test_validation().run(train_ds, test_ds)`**: Runs the **train-test validation** on the `train_ds` and `test_ds` dataset objects. The results are stored in the `res` variable.\n",
        "- **`res.show()`**: Displays the results of the validation, showing if the model's performance on the test set is consistent with its performance on the training set.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The output will display the results of the train-test validation, highlighting whether there is any significant performance drop between the training and testing datasets. The model is expected to perform similarly on both datasets if it is generalizing well.\n",
        "  - If there is a **large drop** in performance from training to testing, it may indicate **overfitting**.\n",
        "\n"
      ],
      "metadata": {
        "id": "-CR5pBVPsbeW"
      },
      "id": "-CR5pBVPsbeW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4e9b34f",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "fee9ea6ae4834f95b12ac36321ddfdd6",
            "4d166e852bc042c1847d62baf9a57308"
          ]
        },
        "id": "a4e9b34f",
        "outputId": "3383f275-7341-4d8c-feb3-23910c54b0c1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "        progress {\n",
              "            -webkit-appearance: none;\n",
              "            border: none;\n",
              "            border-radius: 3px;\n",
              "            width: 300px;\n",
              "            height: 20px;\n",
              "            vertical-align: middle;\n",
              "            margin-right: 10px;\n",
              "            background-color: aliceblue;\n",
              "        }\n",
              "        progress::-webkit-progress-bar {\n",
              "            border-radius: 3px;\n",
              "            background-color: aliceblue;\n",
              "        }\n",
              "        progress::-webkit-progress-value {\n",
              "            background-color: #9d60fb;\n",
              "        }\n",
              "        progress::-moz-progress-bar {\n",
              "            background-color: #9d60fb;\n",
              "        }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d166e852bc042c1847d62baf9a57308",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_VO0OTK59R4V37CVPFSDZOEUDX\">Train Test Validatâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "res = train_test_validation().run(train_ds, test_ds)\n",
        "res.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 16: Check if All Validation Conditions are Passed\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are checking whether all the validation conditions from the **train-test validation** suite have passed. This helps us quickly confirm if the model's performance is consistent and if there are no issues like overfitting or significant degradation between the training and test sets.\n",
        "\n",
        "**Explanation**:\n",
        "- **`all(res.results[i].passed_conditions() for i in range(len(res.results)))`**: This line checks whether all conditions in the `train_test_validation` results have passed. It loops through each result in `res.results` and checks the `passed_conditions()` method, which returns `True` if the validation condition for that result has passed.\n",
        "  - **`res.results[i]`**: Refers to each individual result from the train-test validation suite.\n",
        "  - **`passed_conditions()`**: This method returns `True` if the specific condition (e.g., performance consistency) is met, and `False` if it is not.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The result will be `True` if all the conditions have passed, meaning the model's performance on the test set is consistent with the training set.\n",
        "- If any condition fails (e.g., a significant performance drop), it will return `False`, and you may need to take corrective actions like adjusting the model or revisiting the data preprocessing steps.\n",
        "\n"
      ],
      "metadata": {
        "id": "rBexgY60sllH"
      },
      "id": "rBexgY60sllH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9e5ab2d",
      "metadata": {
        "id": "c9e5ab2d",
        "outputId": "ff0f6bdd-d20b-48e1-b9f8-ea72bc93628a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all(res.results[i].passed_conditions() for i in range(len(res.results)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5d57d7a",
      "metadata": {
        "id": "a5d57d7a"
      },
      "source": [
        "\n",
        "### Step 17: Train the CatBoost Classifier Model\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are training a **CatBoost Classifier** model on the training data (`train_df`) to predict whether fraud was found (`FraudFound_P`). CatBoost is a powerful gradient boosting algorithm that handles categorical features natively.\n",
        "\n",
        "**Explanation**:\n",
        "- **`model = CatBoostClassifier(iterations=100, random_seed=42, verbose=0)`**:\n",
        "  - **`iterations=100`**: Specifies the number of boosting iterations (trees) the model should run. In this case, it will run 100 iterations.\n",
        "  - **`random_seed=42`**: Sets a random seed for reproducibility, ensuring that the results can be replicated.\n",
        "  - **`verbose=0`**: Suppresses the output of each iteration during model training. If set to `1`, it would print progress information for each iteration.\n",
        "  \n",
        "- **`model.fit(train_df.drop(columns=['FraudFound_P', 'Year', 'PolicyNumber']), train_df['FraudFound_P'], cat_features=cat_cols)`**:\n",
        "  - **`train_df.drop(columns=['FraudFound_P', 'Year', 'PolicyNumber'])`**: Drops the columns `FraudFound_P`, `Year`, and `PolicyNumber` from the training data since they are either irrelevant to the model or are the target variable (`FraudFound_P`).\n",
        "  - **`train_df['FraudFound_P']`**: Specifies the target variable, `FraudFound_P`, which is the column that we are trying to predict (whether fraud was found).\n",
        "  - **`cat_features=cat_cols`**: Informs the model that the specified columns in `cat_cols` are categorical features, which helps CatBoost handle them properly.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The CatBoost model will be trained on the provided training data (`train_df`) using the specified parameters.\n",
        "- After training, the model will be ready to make predictions on new data (e.g., the test dataset).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60adc075",
      "metadata": {
        "id": "60adc075",
        "outputId": "2650e4bc-0f1d-4162-c2c7-6c0222740286"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x16c60469b20>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "model = CatBoostClassifier(iterations=100, random_seed=42, verbose=0)\n",
        "model.fit(train_df.drop(columns=['FraudFound_P', 'Year', 'PolicyNumber']), train_df['FraudFound_P'],\n",
        "          cat_features=cat_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 18: Evaluate the Model Using Deepchecks\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are using **Deepchecks'** `model_evaluation` suite to evaluate the performance of the trained CatBoost model on both the training and testing datasets. This step will help us assess how well the model generalizes to unseen data and check if it has overfitted to the training data.\n",
        "\n",
        "**Explanation**:\n",
        "- **`model_evaluation()`**: This is a **Deepchecks** suite used to evaluate a machine learning modelâ€™s performance. It provides several checks, such as accuracy, confusion matrix, and other performance metrics to assess how well the model is performing.\n",
        "- **`res = model_evaluation().run(train_ds, test_ds, model)`**:\n",
        "  - **`train_ds`**: The training dataset.\n",
        "  - **`test_ds`**: The test dataset.\n",
        "  - **`model`**: The trained **CatBoostClassifier** model.\n",
        "  - The `model_evaluation` suite will evaluate the model on both datasets (train and test) and return the results.\n",
        "- **`res.show()`**: Displays the evaluation results, which typically include performance metrics such as accuracy, F1 score, confusion matrix, and more.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The output will display the evaluation results for the **CatBoost model**, including:\n",
        "  - **Accuracy**: How well the model is predicting fraud in both the training and testing datasets.\n",
        "  - **Confusion Matrix**: A matrix showing true positives, true negatives, false positives, and false negatives, helping to assess the classification performance in more detail.\n",
        "  - **Other Metrics**: Depending on the configuration, other performance metrics may also be included, such as precision, recall, and F1 score.\n"
      ],
      "metadata": {
        "id": "9RTuMSeQs8Ox"
      },
      "id": "9RTuMSeQs8Ox"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc539135",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "eb1d4a28e34544f9b9a65ceff3de5810",
            "ec17b979883e4cb9a8b5817d9a6859e9"
          ]
        },
        "id": "dc539135",
        "outputId": "749583c5-cad1-4a0d-8b68-cb8849a96751"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "        progress {\n",
              "            -webkit-appearance: none;\n",
              "            border: none;\n",
              "            border-radius: 3px;\n",
              "            width: 300px;\n",
              "            height: 20px;\n",
              "            vertical-align: middle;\n",
              "            margin-right: 10px;\n",
              "            background-color: aliceblue;\n",
              "        }\n",
              "        progress::-webkit-progress-bar {\n",
              "            border-radius: 3px;\n",
              "            background-color: aliceblue;\n",
              "        }\n",
              "        progress::-webkit-progress-value {\n",
              "            background-color: #9d60fb;\n",
              "        }\n",
              "        progress::-moz-progress-bar {\n",
              "            background-color: #9d60fb;\n",
              "        }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec17b979883e4cb9a8b5817d9a6859e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_1P6HDU6I87FF5X5226LHS1N8X\">Model Evaluation Sâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from deepchecks.tabular.suites import model_evaluation\n",
        "\n",
        "res = model_evaluation().run(train_ds, test_ds, model)\n",
        "res.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 19: Create and Run a Custom Performance Suite\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are creating a custom performance suite using **Deepchecks** to evaluate the model in more detail. The custom suite includes multiple checks, such as train-test performance, boosting overfitting, and confusion matrix. This comprehensive evaluation ensures the model is performing well and is not overfitting.\n",
        "\n",
        "**Explanation**:\n",
        "- **`Suite('My Custom Performance Suite', ...)`**: Creates a custom suite with a name ('My Custom Performance Suite') and adds several checks to evaluate the model's performance thoroughly.\n",
        "  \n",
        "  - **`TrainTestPerformance().add_condition_train_test_relative_degradation_less_than(0.1)`**: This check ensures that the modelâ€™s performance degradation between the training and test datasets is less than 10%. If the performance drops by more than 10%, it may indicate overfitting or that the model is not generalizing well to new data.\n",
        "  \n",
        "  - **`ConfusionMatrixReport()`**: This check generates a confusion matrix, which shows how well the model is performing in classifying fraud cases (true positives, false positives, true negatives, false negatives).\n",
        "  \n",
        "  - **`BoostingOverfit(alternative_scorer=['f1', 'f1']).add_condition_test_score_percent_decline_less_than(0.01)`**: This checks whether the model is overfitting, specifically for boosting models like CatBoost. It ensures that the performance on the test set does not decline by more than 1% compared to the training set.\n",
        "  \n",
        "  - **`SimpleModelComparison().add_condition_gain_greater_than(0.1)`**: This compares the performance of the trained model with a simple baseline model, ensuring that the complex model is performing significantly better than the baseline (greater than 10% improvement).\n",
        "\n",
        "- **`custom_suite.run(train_ds, test_ds, model)`**: Runs the custom suite using the `train_ds`, `test_ds`, and the trained model. This evaluates the model on multiple conditions defined above.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The custom performance suite will generate a detailed report, showing the results for each of the checks:\n",
        "  - **Train-Test Performance**: Ensures that the modelâ€™s performance degradation is minimal.\n",
        "  - **Confusion Matrix**: Provides a breakdown of classification performance (e.g., accuracy, false positives, and true positives).\n",
        "  - **Boosting Overfitting**: Checks if the model has overfitted during training.\n",
        "  - **Simple Model Comparison**: Compares the performance of the complex model with a simple baseline model to ensure that the complexity is justified.\n",
        "\n"
      ],
      "metadata": {
        "id": "D9LFMRVbtF63"
      },
      "id": "D9LFMRVbtF63"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8adf2ba4",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7364fa2742ee426e97bc463ab8753640",
            "a2f9fe2a9b0949f6bff3173320213748"
          ]
        },
        "id": "8adf2ba4",
        "outputId": "0e2c4a01-34eb-447d-c1a0-11f7302fe73e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "        progress {\n",
              "            -webkit-appearance: none;\n",
              "            border: none;\n",
              "            border-radius: 3px;\n",
              "            width: 300px;\n",
              "            height: 20px;\n",
              "            vertical-align: middle;\n",
              "            margin-right: 10px;\n",
              "            background-color: aliceblue;\n",
              "        }\n",
              "        progress::-webkit-progress-bar {\n",
              "            border-radius: 3px;\n",
              "            background-color: aliceblue;\n",
              "        }\n",
              "        progress::-webkit-progress-value {\n",
              "            background-color: #9d60fb;\n",
              "        }\n",
              "        progress::-moz-progress-bar {\n",
              "            background-color: #9d60fb;\n",
              "        }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2f9fe2a9b0949f6bff3173320213748",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_P14H5HTWVXGQVEN9L201SRK6W\">My Custom Performaâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from deepchecks.tabular import Suite\n",
        "from deepchecks.tabular.checks import TrainTestPerformance, BoostingOverfit, SimpleModelComparison, ConfusionMatrixReport\n",
        "\n",
        "custom_suite = Suite('My Custom Performance Suite',\n",
        "                     TrainTestPerformance().add_condition_train_test_relative_degradation_less_than(0.1),\n",
        "                     ConfusionMatrixReport(),\n",
        "                     BoostingOverfit(alternative_scorer=['f1', 'f1']).add_condition_test_score_percent_decline_less_than(0.01),\n",
        "                     SimpleModelComparison().add_condition_gain_greater_than(0.1),\n",
        "                    )\n",
        "\n",
        "custom_suite.run(train_ds, test_ds, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 20: Train a Tuned CatBoost Classifier Model\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are training a **CatBoost Classifier** model with hyperparameters adjusted for better performance. The hyperparameters are tuned to control the modelâ€™s learning rate, complexity, and other aspects to improve generalization and prevent overfitting.\n",
        "\n",
        "**Explanation**:\n",
        "- **`model = CatBoostClassifier(...)`**: We define the model with the following hyperparameters:\n",
        "  - **`iterations=50`**: Specifies the number of boosting iterations (trees). Here, we use 50 iterations to balance model training time and performance.\n",
        "  - **`random_seed=42`**: This sets the random seed for reproducibility, ensuring the results are consistent across different runs.\n",
        "  - **`verbose=0`**: Disables verbose output, so the training process does not display progress for each iteration.\n",
        "  - **`learning_rate=0.2`**: The learning rate controls how much the modelâ€™s weights are updated in each iteration. A lower learning rate can improve model generalization but may require more iterations to converge.\n",
        "  - **`colsample_bylevel=0.03`**: Specifies the fraction of features to sample at each level of the boosting trees. This helps in preventing overfitting by ensuring that the model does not use all features at every split.\n",
        "  - **`subsample=0.5`**: The fraction of samples to be used for each tree. This introduces randomness in the training process and reduces the likelihood of overfitting.\n",
        "  - **`depth=4`**: The maximum depth of the trees. Limiting tree depth can help reduce overfitting, especially when the model is complex.\n",
        "\n",
        "- **`model.fit(...)`**:\n",
        "  - **`train_df.drop(columns=['FraudFound_P', 'Year', 'PolicyNumber'])`**: This removes the target variable (`FraudFound_P`), `Year`, and `PolicyNumber` columns from the training data because they are not needed for model training.\n",
        "  - **`train_df['FraudFound_P']`**: This specifies the target variable, which is what the model is trying to predict (whether fraud is found).\n",
        "  - **`cat_features=cat_cols`**: Specifies which columns are categorical, so **CatBoost** can handle them properly without the need for one-hot encoding.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The **CatBoost** model will be trained on the modified training dataset (`train_df`), and it will attempt to predict whether fraud was found based on the specified hyperparameters.\n",
        "- The model will take into account categorical features and apply the tuned settings to ensure better performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "t1CYUYXht2f0"
      },
      "id": "t1CYUYXht2f0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03475cd0",
      "metadata": {
        "id": "03475cd0",
        "outputId": "c3a1a543-208c-47da-c9e0-4309cdce2fa3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x16c5fc0ceb0>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = CatBoostClassifier(iterations=50, random_seed=42, verbose=0, learning_rate=0.2, colsample_bylevel=0.03, subsample=0.5,\n",
        "                       depth=4)\n",
        "\n",
        "model.fit(train_df.drop(columns=['FraudFound_P', 'Year', 'PolicyNumber']), train_df['FraudFound_P'],\n",
        "          cat_features=cat_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 21: Drop Rows with NaN Values in Specific Columns\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are removing rows from the `data_clean_df` dataset that contain **NaN (missing) values** in specific columns (`Days_Policy_Accident`, `Days_Policy_Claim`, `PastNumberOfClaims`). This is necessary to ensure that we are working with a complete dataset, as many machine learning algorithms cannot handle missing values directly.\n",
        "\n",
        "**Explanation**:\n",
        "- **`data_clean_df.dropna(subset=['Days_Policy_Accident', 'Days_Policy_Claim', 'PastNumberOfClaims'], inplace=True)`**:\n",
        "  - **`subset=['Days_Policy_Accident', 'Days_Policy_Claim', 'PastNumberOfClaims']`**: Specifies that rows containing NaN values in any of these columns will be removed. The `dropna()` function will only drop rows where at least one of the specified columns contains a missing value.\n",
        "  - **`inplace=True`**: This modifies the `data_clean_df` DataFrame directly, removing the rows from the original dataset.\n",
        "\n",
        "- **`print(data_clean_df.isna().sum())`**:\n",
        "  - **`data_clean_df.isna()`**: Returns a DataFrame of the same shape as `data_clean_df` with `True` where a value is missing (NaN) and `False` otherwise.\n",
        "  - **`.sum()`**: Sums the `True` values for each column, which gives the number of missing (NaN) values in each column.\n",
        "  - **`print()`**: Displays the number of remaining NaN values for each column in the dataset.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The rows with NaN values in the specified columns will be removed, and the output will show the count of remaining missing values in each column.\n",
        "  - **Expected Output**: After running the code, you should see that the columns `Days_Policy_Accident`, `Days_Policy_Claim`, and `PastNumberOfClaims` no longer contain any NaN values. Other columns might still have missing values depending on the dataset.\n",
        "\n",
        "\n",
        "**Note**:  \n",
        "Dropping rows with missing values is a common data cleaning step. However, it can lead to a loss of data if many rows have missing values in the specified columns. Alternatively, you could consider imputing missing values rather than removing rows, depending on the nature of the dataset.\n"
      ],
      "metadata": {
        "id": "O633Dg9AuGQ2"
      },
      "id": "O633Dg9AuGQ2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8b8ae2d-b7f0-441b-89a2-9af392b16059",
      "metadata": {
        "scrolled": true,
        "id": "b8b8ae2d-b7f0-441b-89a2-9af392b16059",
        "outputId": "b4d20047-3c6c-4167-de69-a02296de2753"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Month                   0\n",
            "WeekOfMonth             0\n",
            "DayOfWeek               0\n",
            "Make                    0\n",
            "AccidentArea            0\n",
            "DayOfWeekClaimed        0\n",
            "MonthClaimed            0\n",
            "WeekOfMonthClaimed      0\n",
            "Sex                     0\n",
            "MaritalStatus           0\n",
            "Age                     0\n",
            "Fault                   0\n",
            "PolicyType              0\n",
            "VehicleCategory         0\n",
            "VehiclePrice            0\n",
            "PolicyNumber            0\n",
            "RepNumber               0\n",
            "Deductible              0\n",
            "DriverRating            0\n",
            "Days_Policy_Accident    0\n",
            "Days_Policy_Claim       0\n",
            "PastNumberOfClaims      0\n",
            "AgeOfVehicle            0\n",
            "AgeOfPolicyHolder       0\n",
            "PoliceReportFiled       0\n",
            "WitnessPresent          0\n",
            "AgentType               0\n",
            "NumberOfSuppliments     0\n",
            "AddressChange_Claim     0\n",
            "NumberOfCars            0\n",
            "Year                    0\n",
            "BasePolicy              0\n",
            "FraudFound_P            0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Dropping rows with NaN values in specific columns\n",
        "data_clean_df.dropna(subset=['Days_Policy_Accident', 'Days_Policy_Claim', 'PastNumberOfClaims'], inplace=True)\n",
        "\n",
        "# Verify that the rows were dropped\n",
        "print(data_clean_df.isna().sum())  # Check how many NaN values remain in each column\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 22: Clean the Data and Run Full Validation Suite\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are cleaning the training and testing datasets by dropping rows with missing values in specific columns (`Days_Policy_Accident`, `Days_Policy_Claim`, `PastNumberOfClaims`). After cleaning the data, we re-create the **Deepchecks Dataset** objects and run the **full validation suite** to evaluate the overall quality of the datasets and model performance.\n",
        "\n",
        "**Explanation**:\n",
        "- **`train_df = train_ds.data` and `test_df = test_ds.data`**:\n",
        "  - These lines extract the underlying data from the `train_ds` and `test_ds` dataset objects created earlier. These DataFrames are used for further manipulation and cleaning.\n",
        "  \n",
        "- **`train_df.dropna(subset=['Days_Policy_Accident', 'Days_Policy_Claim', 'PastNumberOfClaims'], inplace=True)`**:\n",
        "  - Drops rows with missing values in the specified columns in the training data (`train_df`).\n",
        "  \n",
        "- **`test_df.dropna(subset=['Days_Policy_Accident', 'Days_Policy_Claim', 'PastNumberOfClaims'], inplace=True)`**:\n",
        "  - Drops rows with missing values in the specified columns in the testing data (`test_df`).\n",
        "\n",
        "- **Re-create the Dataset objects**: After cleaning the data, we need to re-create the **Deepchecks Dataset** objects (`train_ds` and `test_ds`), which will be used for validation and model evaluation.\n",
        "  - **`train_ds = Dataset(train_df, label='FraudFound_P', datetime_name='Year', cat_features=cat_cols)`**: Recreates the training dataset object after cleaning.\n",
        "  - **`test_ds = Dataset(test_df, label='FraudFound_P', datetime_name='Year', cat_features=cat_cols)`**: Recreates the test dataset object after cleaning.\n",
        "\n",
        "- **`result = full_suite().run(train_ds, test_ds)`**:\n",
        "  - This line runs the **Deepchecks full suite** on the cleaned `train_ds` and `test_ds` dataset objects. The full suite includes a variety of validation checks, such as data integrity, model evaluation, and performance analysis.\n",
        "  \n",
        "- **`result.show()`**: Displays the results of the full suite validation, including any issues related to the dataset and model performance.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The output will display the results of the **full suite**, which will include various checks like:\n",
        "  - **Data Integrity**: Checks for missing values, data types, correlations, and outliers.\n",
        "  - **Model Evaluation**: Performance metrics such as accuracy, F1 score, and confusion matrix.\n",
        "  - **Train-Test Performance**: Ensures that the model generalizes well from training data to test data.\n",
        "\n",
        "**Note**:  \n",
        "The **full suite** gives a comprehensive view of the modelâ€™s performance and the data's quality, helping you identify any potential issues that might impact model accuracy or reliability. If any issues are found, they should be addressed to ensure the model is working optimally.\n"
      ],
      "metadata": {
        "id": "6UoaJMHouRRe"
      },
      "id": "6UoaJMHouRRe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3553a53b-a7e1-4060-bdcd-e3147fa08d5f",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f6844918d3a34a4e8dcdc06ecb7f01f5"
          ]
        },
        "id": "3553a53b-a7e1-4060-bdcd-e3147fa08d5f",
        "outputId": "3f9cb126-ecb3-46a3-af93-4a4a623c84d7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "        progress {\n",
              "            -webkit-appearance: none;\n",
              "            border: none;\n",
              "            border-radius: 3px;\n",
              "            width: 300px;\n",
              "            height: 20px;\n",
              "            vertical-align: middle;\n",
              "            margin-right: 10px;\n",
              "            background-color: aliceblue;\n",
              "        }\n",
              "        progress::-webkit-progress-bar {\n",
              "            border-radius: 3px;\n",
              "            background-color: aliceblue;\n",
              "        }\n",
              "        progress::-webkit-progress-value {\n",
              "            background-color: #9d60fb;\n",
              "        }\n",
              "        progress::-moz-progress-bar {\n",
              "            background-color: #9d60fb;\n",
              "        }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6844918d3a34a4e8dcdc06ecb7f01f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_6MQKD6649POJXA6H6VBEML4X3\">Full Suite</h1>\\n<â€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from deepchecks.tabular.suites import full_suite\n",
        "from deepchecks.tabular import Dataset\n",
        "\n",
        "# Dropping rows with NaN values in specific columns for both train and test datasets\n",
        "train_df = train_ds.data\n",
        "test_df = test_ds.data\n",
        "\n",
        "# Dropping rows with NaN values in specific columns in both train and test datasets\n",
        "train_df.dropna(subset=['Days_Policy_Accident', 'Days_Policy_Claim', 'PastNumberOfClaims'], inplace=True)\n",
        "test_df.dropna(subset=['Days_Policy_Accident', 'Days_Policy_Claim', 'PastNumberOfClaims'], inplace=True)\n",
        "\n",
        "# Re-create the Dataset objects after cleaning the data\n",
        "train_ds = Dataset(train_df, label='FraudFound_P', datetime_name='Year', cat_features=cat_cols)\n",
        "test_ds = Dataset(test_df, label='FraudFound_P', datetime_name='Year', cat_features=cat_cols)\n",
        "\n",
        "# Run the full suite on the cleaned datasets\n",
        "result = full_suite().run(train_ds, test_ds)\n",
        "result.show()  # This will display the results of the suite\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 23: Run a Custom Performance Suite\n",
        "\n",
        "**What We Are Doing in This Step**:  \n",
        "We are creating and running a **custom performance suite** to evaluate the model's performance across various checks. The custom suite includes several important evaluations, such as train-test performance, overfitting checks, confusion matrix, and model comparison. This step ensures that the model is not only performing well but is also generalizing effectively to unseen data.\n",
        "\n",
        "**Explanation**:\n",
        "- **`Suite('My Custom Performance Suite', ...)`**: We define a custom suite with the name `'My Custom Performance Suite'`. This suite contains multiple checks to evaluate the model in-depth.\n",
        "  - **`TrainTestPerformance().add_condition_train_test_relative_degradation_less_than(0.1)`**: Checks that the relative degradation between the training and testing performance is less than 10%. This ensures that the model generalizes well from training data to test data.\n",
        "  - **`ConfusionMatrixReport()`**: Provides a confusion matrix, which breaks down the modelâ€™s predictions into true positives, true negatives, false positives, and false negatives. This helps assess the accuracy of the modelâ€™s classifications.\n",
        "  - **`BoostingOverfit(alternative_scorer=['f1', 'f1']).add_condition_test_score_percent_decline_less_than(0.01)`**: Ensures that the model, specifically a boosting model like CatBoost, does not overfit. This check compares the modelâ€™s performance on the test set with the training set and makes sure the test score does not decline by more than 1%.\n",
        "  - **`SimpleModelComparison().add_condition_gain_greater_than(0.1)`**: Compares the trained model with a simple baseline model to ensure the complex model performs better by at least 10% compared to the baseline.\n",
        "\n",
        "- **`custom_suite.run(train_ds, test_ds, model)`**: This line runs the custom suite using the `train_ds`, `test_ds`, and the trained model. It will evaluate the model based on the checks defined in the suite.\n",
        "\n",
        "ðŸ”½ Run this cell\n",
        "\n",
        "**Result**:\n",
        "- The output will display a report from the custom performance suite, including:\n",
        "  - **Train-Test Performance**: Shows whether the model's performance degradation between training and testing is acceptable.\n",
        "  - **Confusion Matrix**: Provides a breakdown of how well the model is classifying fraud cases.\n",
        "  - **Boosting Overfitting**: Assesses whether the model is overfitting by comparing the performance on training and test data.\n",
        "  - **Simple Model Comparison**: Compares the performance of the trained model to a simple baseline to ensure that the model adds value.\n",
        "\n",
        "**Note**:  \n",
        "This custom suite allows you to perform a comprehensive evaluation of the modelâ€™s performance. It ensures the model is well-generalized, not overfitting, and performs better than a simple baseline. If any conditions are not met, you may need to revisit model tuning or data preprocessing to improve performance.\n"
      ],
      "metadata": {
        "id": "UfprmAw8uaOs"
      },
      "id": "UfprmAw8uaOs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f647cb",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "1720bd29381c435abccddcd1389e51a2",
            "68e2645013604bba9f7fc32c92f3e059"
          ]
        },
        "id": "72f647cb",
        "outputId": "8ef7038c-39ff-4618-f4e2-d2075b5cc50b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "        progress {\n",
              "            -webkit-appearance: none;\n",
              "            border: none;\n",
              "            border-radius: 3px;\n",
              "            width: 300px;\n",
              "            height: 20px;\n",
              "            vertical-align: middle;\n",
              "            margin-right: 10px;\n",
              "            background-color: aliceblue;\n",
              "        }\n",
              "        progress::-webkit-progress-bar {\n",
              "            border-radius: 3px;\n",
              "            background-color: aliceblue;\n",
              "        }\n",
              "        progress::-webkit-progress-value {\n",
              "            background-color: #9d60fb;\n",
              "        }\n",
              "        progress::-moz-progress-bar {\n",
              "            background-color: #9d60fb;\n",
              "        }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "68e2645013604bba9f7fc32c92f3e059",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_NYT8MXBCZ2XSRWLGFYJ4FJMGQ\">My Custom Performaâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "custom_suite = Suite('My Custom Performance Suite',\n",
        "                     TrainTestPerformance().add_condition_train_test_relative_degradation_less_than(0.1),\n",
        "                     ConfusionMatrixReport(),\n",
        "                     BoostingOverfit(alternative_scorer=['f1', 'f1']).add_condition_test_score_percent_decline_less_than(0.01),\n",
        "                     SimpleModelComparison().add_condition_gain_greater_than(0.1),\n",
        "                    )\n",
        "\n",
        "custom_suite.run(train_ds, test_ds, model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 (PyCaret)",
      "language": "python",
      "name": "pycaret_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
